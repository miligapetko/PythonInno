{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAS Innovate Workbench Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imbalanced-learn\n",
    "# %pip install python-dotenv\n",
    "# %pip install sasctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sasviya.ml.tree import ForestClassifier\n",
    "from sasviya.ml.tree import GradientBoostingClassifier as sas_GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "files = os.listdir()\n",
    "\n",
    "# Filter files that end with .csv\n",
    "csv_files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file as a DataFrame\n",
    "bank = pd.read_csv(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying dataframe\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding datasets dimensions\n",
    "print(\"Bank_Train data shape:\", bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Score set is bigger than Train.\n",
    "\n",
    "*DISCUSSION:* Can consider adding synthetic data to Train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding column format\n",
    "bank.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing categorical variables\n",
    "bank[\"AccountID\"] = bank[\"AccountID\"].astype('category')\n",
    "bank[\"Status\"] = bank[\"Status\"].astype('category')\n",
    "bank[\"Customer_Value\"] = bank[\"Customer_Value\"].astype('category')\n",
    "bank[\"Home_Flag\"] = bank[\"Home_Flag\"].astype('category')\n",
    "bank['Activity_Status'] = bank['Activity_Status'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing variable exploration\n",
    "bank.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Demog_Age and AvgSale3Yr_DP have missing data, but missing 20-25% of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding duplicated data\n",
    "bank.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "bank.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_types(df):\n",
    "    # Separate continuous and categorical variables\n",
    "    continuous_vars = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Plot histograms for continuous variables in one figure\n",
    "    plt.figure(figsize=(20, 50))\n",
    "    for i, col in enumerate(continuous_vars):\n",
    "        plt.subplot(len(continuous_vars)//2 + 1, 2, i + 1)\n",
    "        plt.hist(df[col], bins=20, edgecolor='black')\n",
    "        plt.title(f'{col} Distribution (Histogram)')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bar charts for categorical variables in another figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, col in enumerate(categorical_vars):\n",
    "        plt.subplot(len(categorical_vars)//2 + 1, 2, i + 1)\n",
    "        value_counts = df[col].value_counts()\n",
    "        plt.bar(value_counts.index, value_counts.values, color='skyblue')\n",
    "        plt.title(f'{col} Distribution (Bar Chart)')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_types(bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Age is skewed to the left, have some customers under 18- consider dropping unless they are high worth customers.\n",
    "* AvgSale3Yr, AvgSale3Yr_DP, AvgSaleLife and LastProdAmt are all highly concentrated in one bucket with samll number of positive outliers.\n",
    "* Cnt1Yr_DP, CntPur3Yr, CntPur3Yr_DP, CntPurLife, CntPurLife_DP, CntTotPromo, CustTenure are all skewed to the right.\n",
    "* Imbalanced dataset, with about 20% of data buying the insurance product and 80% not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numeric columns\n",
    "numeric_cols = bank.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Calculate mean of numeric columns\n",
    "mean_values = bank[numeric_cols].mean()\n",
    "\n",
    "# Fill missing values in numeric columns with their respective means\n",
    "bank[numeric_cols] = bank[numeric_cols].fillna(mean_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable encoding\n",
    "\n",
    "* did not use one-hot encoding because want to preserve ordinality of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical columns excluding 'Activity_Status' and 'Customer_Value'\n",
    "categorical_cols = [col for col in bank.select_dtypes(include=['category']).columns if col != 'Activity_Status' and col != 'Customer_Value']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode each categorical column using LabelEncoder\n",
    "for col in categorical_cols:\n",
    "    bank[col] = label_encoder.fit_transform(bank[col])\n",
    "\n",
    "# Mapping for 'Activity_Status' and 'Customer_Value'\n",
    "label_encoding = {\n",
    "    'Activity_Status': {'High': 0, 'Average': 1, 'Low': 2},\n",
    "    'Customer_Value': {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "for col, mapping in label_encoding.items():\n",
    "    bank[col] = bank[col].map(mapping).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking result of parsing and imputation\n",
    "bank.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bank.drop(['Status'], axis=1)\n",
    "y = bank['Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking split results\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python sklearn models\n",
    "\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree, max 3 layers\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, dt.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_valid, dt.predict(X_valid))\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report = classification_report(y_valid, dt.predict(X_valid), output_dict=True)\n",
    "pd.DataFrame(test_report).T[\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "dt_feat = pd.DataFrame(dt.feature_importances_, index=X_train.columns, columns=['feat_importance'])\n",
    "dt_feat.sort_values('feat_importance').tail(8).plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest, with SMOTE\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#    ('scaler', StandardScaler()),\n",
    "#    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('SMOTEENN', SMOTE(random_state=42)),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, pipeline.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_valid, pipeline.predict(X_valid))\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "test_report = classification_report(y_valid, pipeline.predict(X_valid), output_dict=True)\n",
    "print(pd.DataFrame(test_report).T[\"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting, with SMOTE\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.7,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#    ('scaler', StandardScaler()),\n",
    "#    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('SMOTEENN', SMOTE(random_state=42)),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, pipeline.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_valid, pipeline.predict(X_valid))\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "test_report = classification_report(y_valid, pipeline.predict(X_valid), output_dict=True)\n",
    "print(pd.DataFrame(test_report).T[\"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAS Models\n",
    "\n",
    "* Decision Tree\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAS Gradient Boosting, with SMOTE\n",
    "\n",
    "model = sas_GradientBoostingClassifier( # not to be confused with sklearn GB\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.7,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "#    ('scaler', StandardScaler(with_mean=True)),\n",
    "#    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('SMOTEENN', SMOTE(random_state=42)),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "pipeline2.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, pipeline2.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_valid, pipeline2.predict(X_valid))\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "test_report = classification_report(y_valid, pipeline2.predict(X_valid), output_dict=True)\n",
    "pd.DataFrame(test_report).T[\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAS Decision Tree, with SMOTE\n",
    "\n",
    "model = ForestClassifier(\n",
    "    random_state=70\n",
    ")\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "#    ('scaler', StandardScaler(with_mean=True)),\n",
    "#    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('SMOTEENN', SMOTE(random_state=42)),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "pipeline2.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, pipeline2.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_valid, pipeline2.predict(X_valid))\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "test_report = classification_report(y_valid, pipeline2.predict(X_valid), output_dict=True)\n",
    "pd.DataFrame(test_report).T[\"recall\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting pipeline objects to .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn model\n",
    "# dump the pipeline object\n",
    "with open('sklearn_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "\n",
    "del pipeline\n",
    "\n",
    "# load the pipeline object\n",
    "with open('./sklearn_pipeline.pkl', mode='rb') as file:\n",
    "    pipeline = pickle.load(file)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAS model\n",
    "# dump the pipeline object\n",
    "with open('sas_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline2, file)\n",
    "\n",
    "del pipeline\n",
    "\n",
    "# load the pipeline object\n",
    "with open('./sas_pipeline.pkl', mode='rb') as file:\n",
    "    pipeline2 = pickle.load(file)\n",
    "pipeline2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying SAS model manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get access token for viya env using refresh token. change to your own viya server and preferred authentication method.\n",
    "url = \"https://apgtps2demo.gtp.unx.sas.com\"\n",
    "auth_url = f\"{url}/SASLogon/oauth/token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sasctl\n",
    "from sasctl import Session\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading long-lived refresh token from txt file\n",
    "\n",
    "payload=f'grant_type=refresh_token&refresh_token={refresh_token}'\n",
    "headers = {\n",
    "  'Accept': 'application/json',\n",
    "  'Content-Type': 'application/x-www-form-urlencoded',\n",
    "  'Authorization': 'Basic c2FzLmNsaTo=',\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", auth_url, headers=headers, data=payload, verify=False)\n",
    "access_token = response.json()['access_token']\n",
    "\n",
    "st = Session(\"https://apgtps2demo.gtp.unx.sas.com\", token=access_token, verify_ssl=False)\n",
    "st"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workbench Python",
   "language": "python",
   "name": "workbench_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
